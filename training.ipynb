{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "#choose kernel (keras-env) when run on Macbook Pro \n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.profiler\n",
    "import torch.utils.data\n",
    "import torchvision.datasets\n",
    "import torchvision.models\n",
    "import torchvision.transforms as T\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import toml\n",
    "import h5py\n",
    "from keramss import *\n",
    "import joblib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Config, seeding and data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "OHM=True\n",
      "density_threshold=0\n"
     ]
    }
   ],
   "source": [
    "with open('./model_config.toml','r') as f:\n",
    "        config = toml.load(f)\n",
    "        seed = config['stat']['seed']\n",
    "        t1 = config['data']['t1']\n",
    "        t2 = config['data']['t2']\n",
    "        sat = config['data']['sat']\n",
    "        density_threshold = config['data']['density_threshold']\n",
    "        name = config['model']['name']\n",
    "        f_train, f_valid, f_test = config['model']['f_train'], config['model']['f_valid'], config['model']['f_test']\n",
    "        shuffle = config['model']['shuffle']\n",
    "        OHM = config['model']['OHM']\n",
    "        epochs = config['model']['epochs']\n",
    "        patience = config['model']['patience']\n",
    "        data_path = config['paths']['data']\n",
    "        saved_models_path = config['paths']['saved_models']\n",
    "        log_path = config['paths']['logs']\n",
    "\n",
    "        Path(f\"{saved_models_path}/{name}\").mkdir(parents=True, exist_ok=True)\n",
    "        with open(f\"{saved_models_path}/{name}/model_config.toml\",'w') as fsave:\n",
    "            toml.dump(config,fsave)\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "print(f\"OHM={OHM}\")\n",
    "print(f\"density_threshold={density_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size of ~0.25% of data (rounded to closest power of 2) : 32768\n"
     ]
    }
   ],
   "source": [
    "dataset = MMS_Dataset(sat,data_path,t1,t2,density_threshold,OHM=OHM)\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [0.8, 0.1, 0.1],generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "batch_size = int(2**(np.round(np.log2(0.25*len(train_data)/100))))\n",
    "print(f\"Batch size of ~0.25% of data (rounded to closest power of 2) : {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_dnn(input_size,output_size,scale,device):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(input_size, activation='relu'),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(output_size)\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=keras.optimizers.Adam(0.0001),metrics=[\n",
    "                    MaxError(scaler=scale,device=device),\n",
    "                    MSE(scaler=scale,device=device),\n",
    "                    keras.metrics.R2Score(\n",
    "                    class_aggregation=None, num_regressors=0, name=\"r2_score\", dtype=None),\n",
    "                    PCC()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "scale = Standard_Scaler(train_data)\n",
    "torch.save(scale,f'{saved_models_path}/{name}/scale.keras')\n",
    "#torch.load()\n",
    "X_train, y_train = scale.transform(dataset=train_data)\n",
    "X_val  , y_val   = scale.transform(dataset=val_data)\n",
    "X_test , y_test  = scale.transform(dataset=test_data)\n",
    "\n",
    "scale.to(device)\n",
    "X_train, y_train = X_train.to(device) , y_train.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = first_dnn(np.shape(X_train)[1]-3,3,scale,device)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \"\n",
    "            \"and mean absolute error is {:7.2f}.\".format(\n",
    "                epoch, logs[\"loss\"], logs['r2_score'], logs['MSE'], logs['max_error']\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTensorBoard(keras.callbacks.Callback):\n",
    "    '''\n",
    "    https://github.com/keras-team/keras/issues/19121\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "        self._path = path\n",
    "        self._writers = {}\n",
    "\n",
    "    def writer(self, writer):\n",
    "        if writer not in self._writers:\n",
    "            import torch.utils.tensorboard\n",
    "            self._writers[writer] = torch.utils.tensorboard.SummaryWriter(os.path.join(self._path, writer))\n",
    "        return self._writers[writer]\n",
    "\n",
    "    def add_logs(self, writer, logs, step):\n",
    "        for key, value in logs.items():\n",
    "            self.writer(writer).add_scalar(key, value, step)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs:\n",
    "            dic = {}\n",
    "            for k, v in logs.items():\n",
    "                if not k.startswith(\"val_\"):\n",
    "                    #print('k :',k)\n",
    "                    #print('v :',v)\n",
    "                    if type(v) != float and len(v)==3:\n",
    "                        dic.update({k+'/x':v[0]})\n",
    "                        dic.update({k+'/y':v[1]})\n",
    "                        dic.update({k+'/z':v[2]})\n",
    "                        dic.update({k+'/global':v.square().mean().sqrt()})\n",
    "                    else:\n",
    "                        dic.update({k: v})\n",
    "                self.add_logs(\"train\",dic, epoch + 1)\n",
    "            if isinstance(getattr(self.model, \"optimizer\", None), keras.optimizers.Optimizer):\n",
    "                #print(self.model.optimizer.learning_rate)\n",
    "                self.add_logs(\"train\", {\"learning_rate\": self.model.optimizer.learning_rate.numpy()}, epoch + 1)\n",
    "            dic = {}\n",
    "            for k, v in logs.items():\n",
    "                if k.startswith(\"val_\"):\n",
    "                    #print('k :',k)\n",
    "                    #print('v :',v)\n",
    "                    if type(v) != float and len(v)==3:\n",
    "                        dic.update({k[4:]+'/x':v[0]})\n",
    "                        dic.update({k[4:]+'/y':v[1]})\n",
    "                        dic.update({k[4:]+'/z':v[2]})\n",
    "                        dic.update({k[4:]+'/global':v.square().mean().sqrt()})\n",
    "                    else:\n",
    "                        dic.update({k[4:]: v})\n",
    "                self.add_logs(\"val\", dic, epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "log_dir = f'{log_path}/{name}_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "filepath= f'{saved_models_path}/{name}/_weights.best.keras'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "earlystopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.34006, saving model to /home/esevegnes/Workspace/Saved_models/rep_no_thresh_XL/_weights.best.keras\n",
      "358/358 - 50s - 140ms/step - MSE: 4.1600 - PCC: 0.8279 - loss: 0.4051 - max_error: 21.5870 - r2_score: 0.5948 - val_MSE: 3.9958 - val_PCC: 0.8570 - val_loss: 0.3401 - val_max_error: 8.8374 - val_r2_score: 0.6598\n",
      "Epoch 2/3000\n",
      "\n",
      "Epoch 2: val_loss improved from 0.34006 to 0.24492, saving model to /home/esevegnes/Workspace/Saved_models/rep_no_thresh_XL/_weights.best.keras\n",
      "358/358 - 50s - 139ms/step - MSE: 3.2631 - PCC: 0.8658 - loss: 0.2799 - max_error: 24.3592 - r2_score: 0.7200 - val_MSE: 0.9203 - val_PCC: 0.9064 - val_loss: 0.2449 - val_max_error: 8.2560 - val_r2_score: 0.7549\n",
      "Epoch 3/3000\n"
     ]
    }
   ],
   "source": [
    "my_callbacks = [earlystopping_callback, checkpoint, TorchTensorBoard(log_dir)]#, LossAndErrorPrintingCallback()]\n",
    "if True:\n",
    "    model.fit(X_train, y_train, epochs=epochs, \n",
    "            validation_data=(X_val, y_val), verbose=2,\n",
    "            callbacks=my_callbacks,batch_size=batch_size)\n",
    "    model.save_weights(f'{saved_models_path}/{name}/model.weights.h5', overwrite=True)\n",
    "    model.save(f'{saved_models_path}/{name}/model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    new_model = keras.models.load_model(f'{saved_models_path}/{name}/model.keras')\n",
    "    model.fit(X_train, y_train, epochs=1000, \n",
    "            validation_data=(X_val, y_val), verbose=2,\n",
    "            callbacks=my_callbacks,batch_size=batch_size, initial_epoch=300)\n",
    "    model.save_weights(f'{saved_models_path}/{name}_XL/model.weights.h5', overwrite=True)\n",
    "    model.save(f'{saved_models_path}/{name}_XL/model.keras'))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
